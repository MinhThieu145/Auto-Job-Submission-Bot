{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import os\n",
    "import subprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some constants\n",
    "BUCKET_NAME = 'job-site-scrapers-home'\n",
    "\n",
    "BUCKET_NAME_RESULT = 'job-scraping-result'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "bucket_name = BUCKET_NAME\n",
    "local_directory = 'scrapers'\n",
    "\n",
    "\n",
    "def download_folder(bucket_name, folder_name, local_dir):\n",
    "    s3 = boto3.client('s3')\n",
    "    response = s3.list_objects_v2(Bucket=bucket_name, Prefix=folder_name)\n",
    "\n",
    "    for obj in response.get('Contents', []):\n",
    "        file_key = obj.get('Key')\n",
    "        local_path = os.path.join(local_dir, file_key)\n",
    "        os.makedirs(os.path.dirname(local_path), exist_ok=True)\n",
    "        s3.download_file(bucket_name, file_key, local_path)\n",
    "\n",
    "def list_s3_top_level_folders(bucket_name):\n",
    "    s3 = boto3.client('s3')\n",
    "    response = s3.list_objects_v2(Bucket=bucket_name, Delimiter='/')\n",
    "\n",
    "    folders = []\n",
    "    for prefix in response.get('CommonPrefixes', []):\n",
    "        folder_name = prefix.get('Prefix')\n",
    "        folders.append(folder_name)\n",
    "\n",
    "    return folders\n",
    "\n",
    "\n",
    "# let clear the local folder before download\n",
    "# get folder called scrapers and empty it\n",
    "subprocess.run(['rm', '-rf', 'scrapers'])\n",
    "subprocess.run(['mkdir', 'scrapers'])\n",
    "\n",
    "folder_list = list_s3_top_level_folders(bucket_name)\n",
    "for folder in folder_list:\n",
    "    download_folder(bucket_name, folder.rstrip('/'), local_directory)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# upload to a bucket called job-scraping-result\n",
    "\n",
    "def UploadJobScrapingResult(folder_name, file, file_path):\n",
    "    s3 = boto3.client('s3')\n",
    "    s3.upload_file(file_path, BUCKET_NAME_RESULT, folder_name + '/' + file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the result inside each scraper folder\n",
    "\n",
    "def GetResultandUploadS3(result_folder_path, folder_name):\n",
    "\n",
    "    # list all the files inside result folder\n",
    "    result_files = os.listdir(result_folder_path)\n",
    "\n",
    "    csv_files = [file for file in result_files if file.endswith('.csv')]\n",
    "\n",
    "    # add the date to folder_name\n",
    "    folder_name = folder_name + '-' + str(pd.datetime.now().date())\n",
    "\n",
    "    # upload the file to s3\n",
    "    for file in csv_files:\n",
    "        file_path = f'{result_folder_path}/{file}'\n",
    "        UploadJobScrapingResult(folder_name=folder_name, file=file, file_path=file_path)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "folder_name: linkedin-scraper\n",
      "/home/thieu/DataScience2/AutoJobSubmittion/run-everyday-stuff/scrapers/linkedin-scraper/result\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_17468/3890162792.py:11: FutureWarning: The pandas.datetime class is deprecated and will be removed from pandas in a future version. Import from datetime module instead.\n",
      "  folder_name = folder_name + '-' + str(pd.datetime.now().date())\n"
     ]
    }
   ],
   "source": [
    "folder_path = os.path.join(os.getcwd(), 'scrapers', folder)\n",
    "\n",
    "# folder name of folder_path\n",
    "folder_name = folder_path.split('/')[-1]\n",
    "print('folder_name:', folder_name)\n",
    "\n",
    "result_folder_path = os.path.join(folder_path, 'result')\n",
    "\n",
    "print(result_folder_path)\n",
    "\n",
    "\n",
    "GetResultandUploadS3(result_folder_path, folder_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "linkedin-scraper\n",
      "Cookie loaded\n",
      "start scraping that\n",
      "what the hell\n",
      "data%20intern\n",
      "Empty DataFrame\n",
      "Columns: [job_title, job_link, description, company_name, company_link, job_location, workplace_type, post_date, number_of_applicants, skill_list, title]\n",
      "Index: []\n",
      "https://www.linkedin.com/jobs/search/?geoId=103644278&keywords=data%20intern&location=United%20States&refresh=true&sortBy=R\n",
      "Show 3 results\n",
      "find job\n",
      "[<selenium.webdriver.remote.webelement.WebElement (session=\"c6ed9544db17a864a4fc32f96b8914bc\", element=\"C23659429FD1E002885C1A221FE32932_element_55\")>, <selenium.webdriver.remote.webelement.WebElement (session=\"c6ed9544db17a864a4fc32f96b8914bc\", element=\"C23659429FD1E002885C1A221FE32932_element_56\")>, <selenium.webdriver.remote.webelement.WebElement (session=\"c6ed9544db17a864a4fc32f96b8914bc\", element=\"C23659429FD1E002885C1A221FE32932_element_57\")>, <selenium.webdriver.remote.webelement.WebElement (session=\"c6ed9544db17a864a4fc32f96b8914bc\", element=\"C23659429FD1E002885C1A221FE32932_element_58\")>]\n",
      "----------------------------------------\n",
      "Start on new page\n",
      "error reading page list or there is only one page\n",
      "3\n",
      "0 - IT Intern, Data and Analytics - OIA GLOBAL - 94 applicants\n",
      "1 - IT Data and Analytics Intern - Fall 2023 Internship - Primient - Over 200 applicants\n",
      "2 - Fall Intern, Data Engineering - Part Time - SiriusXM - \n",
      "\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "GetResultandUploadS3() missing 1 required positional argument: 'folder_name'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[56], line 21\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[39m# get the result folder path inside the folder_path\u001b[39;00m\n\u001b[1;32m     20\u001b[0m result_folder_path \u001b[39m=\u001b[39m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mjoin(folder_path, \u001b[39m'\u001b[39m\u001b[39mresult\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m---> 21\u001b[0m GetResultandUploadS3(result_folder_path)\n",
      "\u001b[0;31mTypeError\u001b[0m: GetResultandUploadS3() missing 1 required positional argument: 'folder_name'"
     ]
    }
   ],
   "source": [
    "# get a folder called scrapers\n",
    "scrapers_folder = os.listdir('scrapers')\n",
    "\n",
    "# loop through all folder inside that scrapers folder\n",
    "for folder in scrapers_folder:\n",
    "    print(folder)\n",
    "\n",
    "    # get the folder path of the folder\n",
    "    folder_path = os.path.join(os.getcwd(), 'scrapers', folder)\n",
    "\n",
    "    # get the script path: main.py in each folder\n",
    "    script_path = os.path.join(folder_path, 'main.py')\n",
    "\n",
    "    # run the script\n",
    "    \n",
    "    result = subprocess.run(['python', script_path], cwd=folder_path, capture_output=True)\n",
    "    print(result.stdout.decode('utf-8'))    \n",
    "\n",
    "    # get the result folder path inside the folder_path\n",
    "    result_folder_path = os.path.join(folder_path, 'result')\n",
    "    GetResultandUploadS3(result_folder_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DataAnalyst",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
